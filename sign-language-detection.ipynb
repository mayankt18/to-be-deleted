{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cek Ditektori"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# input_path = '/kaggle/input'\n",
    "# for dirname, _, filenames in os.walk(input_path):\n",
    "#     print(f\"Folder: {dirname}\")\n",
    "#     for filename in filenames:\n",
    "#         print(f\"- {filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Persiapan Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fungsi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_random_directories(base_dir, class_num):\n",
    "    all_dirs = [os.path.join(base_dir, d) for d in os.listdir(base_dir) if os.path.isdir(os.path.join(base_dir, d))]\n",
    "    return random.sample(all_dirs, min(class_num, len(all_dirs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_all_directories(base_dir):\n",
    "    return [os.path.join(base_dir, d) for d in os.listdir(base_dir) if os.path.isdir(os.path.join(base_dir, d))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_video_data(class_dirs, sequence_length, frame_size):\n",
    "    sequences = []\n",
    "    labels = []\n",
    "    label_map = {}\n",
    "\n",
    "    for i, class_path in enumerate(class_dirs):\n",
    "        class_name = os.path.basename(class_path)\n",
    "        print(f\"Processing class {i}: {class_name}\")\n",
    "        label_map[class_name] = i\n",
    "\n",
    "        for video_name in os.listdir(class_path):\n",
    "            video_path = os.path.join(class_path, video_name)\n",
    "            sequence = process_video(video_path, sequence_length, frame_size)\n",
    "            if sequence is not None:\n",
    "                sequences.append(sequence)\n",
    "                labels.append(i)\n",
    "\n",
    "    return np.array(sequences, dtype=np.float32), np.array(labels), label_map\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_video(video_path, sequence_length, frame_size):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    frames = []\n",
    "\n",
    "    while len(frames) < sequence_length:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        frame = cv2.resize(frame, frame_size)\n",
    "        frame = frame / 255.0\n",
    "        frames.append(frame)\n",
    "\n",
    "    cap.release()\n",
    "\n",
    "    if len(frames) < sequence_length:\n",
    "        return None\n",
    "\n",
    "    if len(frames) > sequence_length:\n",
    "        indices = np.linspace(0, len(frames) - 1, sequence_length, dtype=int)\n",
    "        frames = [frames[i] for i in indices]\n",
    "\n",
    "    return np.array(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_video(video_tensor):\n",
    "    seed = tf.random.uniform(shape=(), maxval=10000, dtype=tf.int32)\n",
    "\n",
    "    video_tensor = tf.image.stateless_random_flip_left_right(video_tensor, seed=[seed, seed])\n",
    "    video_tensor = tf.image.stateless_random_brightness(video_tensor, max_delta=0.2, seed=[seed, seed])\n",
    "    video_tensor = tf.image.stateless_random_contrast(video_tensor, lower=0.8, upper=1.2, seed=[seed, seed])\n",
    "    return video_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_val_split(X, y, val_size, batch_size, random_state=None, augment_fn = None):\n",
    "    if random_state is not None:\n",
    "        np.random.seed(random_state)\n",
    "\n",
    "    indices = np.arange(len(X))\n",
    "    np.random.shuffle(indices)\n",
    "\n",
    "    num_val = int(len(X) * val_size)\n",
    "    train_indices, val_indices = indices[:-num_val], indices[-num_val:]\n",
    "\n",
    "    def generator(indices):\n",
    "        for idx in indices:\n",
    "            yield X[idx], y[idx]\n",
    "\n",
    "    train_dataset = tf.data.Dataset.from_generator(\n",
    "        lambda: generator(train_indices),\n",
    "        output_signature=(tf.TensorSpec(shape=X.shape[1:], dtype=tf.float32),\n",
    "                          tf.TensorSpec(shape=y.shape[1:], dtype=tf.float32))\n",
    "    )\n",
    "    val_dataset = tf.data.Dataset.from_generator(\n",
    "        lambda: generator(val_indices),\n",
    "        output_signature=(tf.TensorSpec(shape=X.shape[1:], dtype=tf.float32),\n",
    "                          tf.TensorSpec(shape=y.shape[1:], dtype=tf.float32))\n",
    "    )\n",
    "\n",
    "    train_dataset = train_dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "    val_dataset = val_dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "    return train_dataset, val_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_video_info(data, num_samples=5):\n",
    "    X, y = data\n",
    "    video_info = []\n",
    "    \n",
    "    random_indices = random.sample(range(len(X)), num_samples)\n",
    "    \n",
    "    for idx in random_indices:\n",
    "        video = X[idx]\n",
    "        label = y[idx]\n",
    "        \n",
    "        video_length = len(video)\n",
    "        frame_size = video.shape[1:]\n",
    "        class_label = label\n",
    "        \n",
    "        first_frame = video[0]\n",
    "        frame_shape = first_frame.shape\n",
    "        \n",
    "        video_info.append({\n",
    "            'video_index': idx,\n",
    "            'video_length': video_length,\n",
    "            'frame_size': frame_size,\n",
    "            'first_frame_shape': frame_shape,\n",
    "            'label': class_label\n",
    "        })\n",
    "    \n",
    "    return video_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variabel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = 'SL1/'\n",
    "CLASS_NUM = 140\n",
    "\n",
    "SEQUENCE_LENGTH = 30\n",
    "FRAME_SIZE = (224, 224)\n",
    "\n",
    "VAL_SIZE = 0.2\n",
    "RANDOM_SEED = 21\n",
    "\n",
    "BATCH_SIZE = 10\n",
    "BUFFER_SIZE = 1000\n",
    "\n",
    "LABEL_LIST = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Proses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_dirs = select_random_directories(BASE_DIR,CLASS_NUM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing class 0: purple\n",
      "Processing class 1: morning\n",
      "Processing class 2: child\n",
      "Processing class 3: doctor\n",
      "Processing class 4: house\n",
      "Processing class 5: two\n",
      "Processing class 6: hello\n"
     ]
    }
   ],
   "source": [
    "X, y, label_map = load_video_data(class_dirs, SEQUENCE_LENGTH, FRAME_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_info = get_random_video_info((X, y))\n",
    "for info in video_info:\n",
    "    print(f\"Video Index: {info['video_index']}\")\n",
    "    print(f\"Video Length: {info['video_length']} frames\")\n",
    "    print(f\"Frame Size: {info['frame_size']}\")\n",
    "    print(f\"First Frame Shape: {info['first_frame_shape']}\")\n",
    "    print(f\"Label: {info['label']}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, val_dataset = train_val_split(X, y, VAL_SIZE, BATCH_SIZE, RANDOM_SEED, augment_video)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pelatihan Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fungsi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(class_num, sequence_length, frame_size):\n",
    "    inputs = layers.Input(shape=(sequence_length, *frame_size, 3))\n",
    "    \n",
    "    x = layers.Conv3D(16, kernel_size=(3, 3, 3), activation='relu', padding='valid')(inputs)\n",
    "    # x = layers.Conv3D(32, kernel_size=(3, 3, 3), activation='relu', padding='valid')(x)\n",
    "    x = layers.MaxPooling3D(pool_size=(2, 2, 2))(x)\n",
    "    x = layers.Dropout(0.2)(x)\n",
    "    \n",
    "    x = layers.Conv3D(32, kernel_size=(3, 3, 3), activation='relu', padding='valid')(x)\n",
    "    # x = layers.Conv3D(64, kernel_size=(3, 3, 3), activation='relu', padding='valid')(x)\n",
    "    x = layers.MaxPooling3D(pool_size=(2, 2, 2))(x)\n",
    "    x = layers.Dropout(0.2)(x)\n",
    "    \n",
    "    x = layers.Conv3D(32, kernel_size=(3, 3, 3), activation='relu', padding='valid')(x)\n",
    "    # x = layers.Conv3D(64, kernel_size=(3, 3, 3), activation='relu', padding='valid')(x)\n",
    "    x = layers.MaxPooling3D(pool_size=(2, 2, 2))(x)\n",
    "    x = layers.Dropout(0.2)(x)\n",
    "\n",
    "    x = layers.Conv3D(32, kernel_size=(3, 3, 3), activation='relu', padding='valid')(x)\n",
    "    # x = layers.Conv3D(64, kernel_size=(3, 3, 3), activation='relu', padding='valid')(x)\n",
    "    # x = layers.MaxPooling3D(pool_size=(2, 2, 2))(x)\n",
    "    x = layers.Dropout(0.2)(x)\n",
    "    \n",
    "    x = layers.Flatten()(x)\n",
    "    x = layers.Dense(96, activation='relu')(x)\n",
    "    x = layers.Dropout(0.4)(x)\n",
    "    \n",
    "    outputs = layers.Dense(class_num, activation='softmax')(x)\n",
    "\n",
    "    model = models.Model(inputs, outputs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_dataset, val_dataset, batch_size, epochs):\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.AdamW(learning_rate=3e-4, weight_decay=5e-2),\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "\n",
    "    history = model.fit(\n",
    "        train_dataset,\n",
    "        validation_data=val_dataset,\n",
    "        epochs=epochs,\n",
    "        callbacks=[]\n",
    "    )\n",
    "\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(history):\n",
    "    loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "    accuracy = history.history['accuracy']\n",
    "    val_accuracy = history.history['val_accuracy']\n",
    "\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "    ax1.plot(loss, label='Training Loss')\n",
    "    ax1.plot(val_loss, label='Validation Loss')\n",
    "    ax1.set_title('Loss')\n",
    "    ax1.set_xlabel('Epochs')\n",
    "    ax1.set_ylabel('Loss')\n",
    "    ax1.legend()\n",
    "\n",
    "    ax2.plot(accuracy, label='Training Accuracy')\n",
    "    ax2.plot(val_accuracy, label='Validation Accuracy')\n",
    "    ax2.set_title('Accuracy')\n",
    "    ax2.set_xlabel('Epochs')\n",
    "    ax2.set_ylabel('Accuracy')\n",
    "    ax2.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Proses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_model(CLASS_NUM, SEQUENCE_LENGTH, FRAME_SIZE)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = train_model(model, train_dataset, val_dataset, BATCH_SIZE, EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"sign_language_video_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(label_map) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label_map = {'train': 0, 'ready': 1, 'hospital': 2, 'school': 3, 'no': 4, 'again': 5, 'chair': 6, 'tomorrow': 7, 'three': 8, 'tired': 9, 'week': 10, 'year': 11, 'black': 12, 'some': 13, 'drive': 14, 'what': 15, 'stand': 16, 'help': 17, 'good': 18, 'slow': 19, 'baby': 20, 'study': 21, 'sit': 22, 'which': 23, 'please': 24, 'red': 25, 'brown': 26, 'blue': 27, 'clothes': 28, 'busy': 29, 'family': 30, 'two': 31, 'shoes': 32, 'need': 33, 'play': 34, 'door': 35, 'more': 36, 'we': 37, 'understand': 38, 'danger': 39, 'morning': 40, 'wait': 41, 'bus': 42, 'month': 43, 'yes': 44, 'bad': 45, 'yellow': 46, 'sleep': 47, 'maybe': 48, 'write': 49, 'yesterday': 50, 'green': 51, 'doctor': 52, 'thank you': 53, 'emergency': 54, 'sorry': 55, 'sister': 56, 'child': 57, 'table': 58, 'excited': 59, 'laugh': 60, 'drink': 61, 'listen': 62, 'all': 63, 'food': 64, 'fine': 65, 'store': 66, 'when': 67, 'eat': 68, 'sad': 69, 'wake up': 70, 'car': 71, 'person': 72, 'angry': 73, 'night': 74, 'goodbye': 75, 'today': 76, 'come': 77, 'read': 78, 'where': 79, 'pink': 80, 'always': 81, 'work': 82, 'father': 83, 'afternoon': 84, 'safe': 85, 'cry': 86, 'buy': 87, 'now': 88, 'you': 89, 'fast': 90, 'walk': 91, 'purple': 92, 'pain': 93, 'run': 94, 'love': 95, 'later': 96, 'mother': 97, 'none': 98, 'who': 99, 'watch': 100, 'brother': 101, 'four': 102, 'sick': 103, 'i': 104, 'soon': 105, 'phone': 106, 'book': 107, 'office': 108, 'window': 109, 'hate': 110, 'hello': 111, 'white': 112, 'they': 113, 'why': 114, 'home': 115, 'she': 116, 'stop': 117, 'restaurant': 118, 'after': 119, 'before': 120, 'how': 121, 'police': 122, 'five': 123, 'me': 124, 'hurt': 125, 'orange': 126, 'water': 127, 'nervous': 128, 'scared': 129, 'talk': 130, 'house': 131, 'computer': 132, 'friend': 133, 'bank': 134, 'one': 135, 'bored': 136, 'happy': 137, 'less': 138, 'go': 139}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "model = load_model(\"sign_language_video_model.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_new_video(video_path, sequence_length=60, frame_size=(112, 112)):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    frames = []\n",
    "    \n",
    "    while len(frames) < sequence_length:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        frame = cv2.resize(frame, frame_size)\n",
    "        frame = frame / 255.0\n",
    "        frames.append(frame)\n",
    "    \n",
    "    cap.release()\n",
    "    # print(len(frames),sequence_length)\n",
    "    if len(frames) < sequence_length:\n",
    "        return None  # Ignore videos that are too short\n",
    "    \n",
    "    if len(frames) > sequence_length:\n",
    "        indices = np.linspace(0, len(frames) - 1, sequence_length, dtype=int)\n",
    "        frames = [frames[i] for i in indices]\n",
    "    \n",
    "    return np.array(frames)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 117ms/step\n",
      "Predicted Class: 111\n"
     ]
    }
   ],
   "source": [
    "video_path = \"SL1/hello/hello_komal.mp4\"  # Replace with the actual video path\n",
    "video_data = preprocess_new_video(video_path)\n",
    "\n",
    "if video_data is not None:\n",
    "    video_data = np.expand_dims(video_data, axis=0)  # Add batch dimension\n",
    "    predictions = model.predict(video_data)\n",
    "    predicted_class = np.argmax(predictions)\n",
    "    \n",
    "    print(f\"Predicted Class: {predicted_class}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Sign Language Gesture: hello\n"
     ]
    }
   ],
   "source": [
    "class_labels = {v: k for k, v in label_map.items()}  # Reverse the label_map\n",
    "predicted_label = class_labels[predicted_class]\n",
    "\n",
    "print(f\"Predicted Sign Language Gesture: {predicted_label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install opencv-python mediapipe tensorflow numpy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred(path):\n",
    "    video_path = path  # Replace with the actual video path\n",
    "    video_data = preprocess_new_video(video_path)\n",
    "    \n",
    "    if video_data is not None:\n",
    "        video_data = np.expand_dims(video_data, axis=0)  # Add batch dimension\n",
    "        predictions = model.predict(video_data)\n",
    "        predicted_class = np.argmax(predictions)\n",
    "        \n",
    "        class_labels = {v: k for k, v in label_map.items()}  # Reverse the label_map\n",
    "        predicted_label = class_labels[predicted_class]\n",
    "        \n",
    "        print(f\"Predicted Sign Language Gesture: {predicted_label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frames per segment: 90\n",
      "Started new segment: video_segments_1588982b-ed03-4909-a786-28bf813eed8f\\segment_0.mp4\n",
      "Error: Failed to capture frame.\n",
      "Captured and saved 1 video segments in the 'video_segments_1588982b-ed03-4909-a786-28bf813eed8f' folder.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import os\n",
    "import uuid\n",
    "import time\n",
    "\n",
    "cap = cv2.VideoCapture('SL1/hello/hello_komal.mp4')\n",
    "\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Could not open video source.\")\n",
    "    exit()\n",
    "\n",
    "# total_duration = 15\n",
    "segment_duration = 3\n",
    "\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "# total_frames = int(fps * total_duration)\n",
    "frames_per_segment = int(fps * segment_duration)\n",
    "print(f\"Frames per segment: {frames_per_segment}\")\n",
    "# print(f\"Total frames: {total_frames}\")\n",
    "\n",
    "output_folder = f'video_segments_{uuid.uuid4()}'\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "frame_count = 0\n",
    "segment_count = 0\n",
    "list = []\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "\n",
    "out = None\n",
    "# while frame_count < total_frames:\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret: \n",
    "        print(\"Error: Failed to capture frame.\")\n",
    "        break\n",
    "    if frame_count % frames_per_segment == 0:\n",
    "        if out is not None:\n",
    "            out.release()\n",
    "        segment_filename = os.path.join(output_folder, f'segment_{segment_count}.mp4')\n",
    "        frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "        frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "        out = cv2.VideoWriter(segment_filename, fourcc, fps, (frame_width, frame_height))\n",
    "        segment_count += 1\n",
    "        print(f\"Started new segment: {segment_filename}\")\n",
    "        list.append(segment_filename)\n",
    "\n",
    "    out.write(frame)\n",
    "    frame_count += 1\n",
    "\n",
    "cap.release()\n",
    "if out is not None:\n",
    "    out.release()\n",
    "\n",
    "print(f\"Captured and saved {segment_count} video segments in the '{output_folder}' folder.\")            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 116ms/step\n",
      "Predicted Sign Language Gesture: hello\n"
     ]
    }
   ],
   "source": [
    "for l in list:\n",
    "    pred(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 2941073,
     "sourceId": 5065469,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30805,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
